{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220543da",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Japanese Audio ‚Üí SRT Subtitles (JP + EN)\n",
    "\n",
    "This notebook:\n",
    "1. **Transcribes** uploaded Japanese audio using [Qwen/Qwen3-ASR-1.7B](https://huggingface.co/Qwen/Qwen3-ASR-1.7B) with word-level timestamps via the ForcedAligner\n",
    "2. **Generates** an SRT subtitle file from the transcription\n",
    "3. **Translates** the Japanese SRT to English using [Helsinki-NLP/opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en)\n",
    "\n",
    "**Requirements:** A Colab runtime with a **T4 GPU** (free tier works).\n",
    "\n",
    "> ‚ö†Ô∏è Make sure you've selected **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7cff5",
   "metadata": {},
   "source": [
    "## 1 ¬∑ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q qwen-asr transformers sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41672cc7",
   "metadata": {},
   "source": [
    "## 2 ¬∑ Upload Japanese Audio File\n",
    "\n",
    "Supported formats: `.wav`, `.mp3`, `.flac`, `.ogg`, `.m4a`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57abc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML\n",
    "import os\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept=\".wav,.mp3,.flac,.ogg,.m4a,.aac,.wma,.opus\",\n",
    "    multiple=False,\n",
    "    description=\"Select audio\",\n",
    "    button_style=\"primary\",\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "status = widgets.HTML(value=\"<i>No file selected.</i>\")\n",
    "\n",
    "AUDIO_PATH = None\n",
    "\n",
    "\n",
    "def on_upload(change):\n",
    "    global AUDIO_PATH\n",
    "    uploaded = change[\"new\"]\n",
    "    if uploaded:\n",
    "        file_info = uploaded[0]\n",
    "        name = file_info[\"name\"]\n",
    "        content = file_info[\"content\"]\n",
    "        AUDIO_PATH = os.path.join(\"/content\", name)\n",
    "        with open(AUDIO_PATH, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        size_mb = len(content) / (1024 * 1024)\n",
    "        status.value = f\"‚úÖ <b>{name}</b> uploaded ({size_mb:.1f} MB)\"\n",
    "\n",
    "\n",
    "uploader.observe(on_upload, names=\"value\")\n",
    "display(widgets.VBox([uploader, status]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd319ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the uploaded audio\n",
    "if AUDIO_PATH and os.path.exists(AUDIO_PATH):\n",
    "    display(Audio(AUDIO_PATH))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please upload an audio file in the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600d6d4",
   "metadata": {},
   "source": [
    "## 3 ¬∑ Transcribe with Qwen3-ASR-1.7B\n",
    "\n",
    "Loads the ASR model and the ForcedAligner to get word-level timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qwen_asr import Qwen3ASRModel\n",
    "\n",
    "assert AUDIO_PATH and os.path.exists(AUDIO_PATH), (\n",
    "    \"No audio file found. Run the upload cell above first.\"\n",
    ")\n",
    "\n",
    "print(\"Loading Qwen3-ASR-1.7B + ForcedAligner ‚Ä¶\")\n",
    "asr_model = Qwen3ASRModel.from_pretrained(\n",
    "    \"Qwen/Qwen3-ASR-1.7B\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    max_inference_batch_size=32,\n",
    "    max_new_tokens=4096,  # long audio support\n",
    "    forced_aligner=\"Qwen/Qwen3-ForcedAligner-0.6B\",\n",
    "    forced_aligner_kwargs=dict(\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",\n",
    "    ),\n",
    ")\n",
    "print(\"‚úÖ Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Transcribing: {os.path.basename(AUDIO_PATH)} ‚Ä¶\")\n",
    "\n",
    "results = asr_model.transcribe(\n",
    "    audio=AUDIO_PATH,\n",
    "    language=\"Japanese\",\n",
    "    return_time_stamps=True,\n",
    ")\n",
    "\n",
    "result = results[0]\n",
    "print(f\"\\nDetected language: {result.language}\")\n",
    "print(f\"Full text:\\n{result.text}\")\n",
    "print(f\"\\nTimestamp segments: {len(result.time_stamps[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3baa9",
   "metadata": {},
   "source": [
    "## 4 ¬∑ Generate SRT Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def format_srt_time(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to SRT timestamp format: HH:MM:SS,mmm\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    secs = total_seconds % 60\n",
    "    millis = int(td.microseconds / 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n",
    "\n",
    "\n",
    "def group_timestamps_to_subtitles(\n",
    "    stamps, max_chars: int = 40, max_duration: float = 7.0, gap_threshold: float = 0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Group word-level timestamps into subtitle segments.\n",
    "\n",
    "    Args:\n",
    "        stamps: list of timestamp objects with .text, .start_time, .end_time\n",
    "        max_chars: max characters per subtitle line\n",
    "        max_duration: max duration (seconds) per subtitle\n",
    "        gap_threshold: silence gap (seconds) that forces a new subtitle\n",
    "    \"\"\"\n",
    "    if not stamps:\n",
    "        return []\n",
    "\n",
    "    subtitles = []\n",
    "    current_text = \"\"\n",
    "    current_start = stamps[0].start_time\n",
    "    current_end = stamps[0].end_time\n",
    "\n",
    "    for i, stamp in enumerate(stamps):\n",
    "        # Decide whether to start a new subtitle\n",
    "        start_new = False\n",
    "        if i == 0:\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "            continue\n",
    "\n",
    "        # Check gap between previous and current word\n",
    "        gap = stamp.start_time - current_end\n",
    "        new_duration = stamp.end_time - current_start\n",
    "        new_len = len(current_text) + len(stamp.text)\n",
    "\n",
    "        if gap > gap_threshold or new_duration > max_duration or new_len > max_chars:\n",
    "            start_new = True\n",
    "\n",
    "        if start_new:\n",
    "            subtitles.append((current_start, current_end, current_text.strip()))\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "        else:\n",
    "            current_text += stamp.text\n",
    "            current_end = stamp.end_time\n",
    "\n",
    "    # Don't forget the last segment\n",
    "    if current_text.strip():\n",
    "        subtitles.append((current_start, current_end, current_text.strip()))\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def build_srt(subtitles) -> str:\n",
    "    \"\"\"Build SRT string from list of (start, end, text) tuples.\"\"\"\n",
    "    lines = []\n",
    "    for idx, (start, end, text) in enumerate(subtitles, 1):\n",
    "        lines.append(str(idx))\n",
    "        lines.append(f\"{format_srt_time(start)} --> {format_srt_time(end)}\")\n",
    "        lines.append(text)\n",
    "        lines.append(\"\")  # blank line separator\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Build Japanese SRT\n",
    "stamps = result.time_stamps[0]\n",
    "subtitles_ja = group_timestamps_to_subtitles(stamps)\n",
    "\n",
    "srt_ja = build_srt(subtitles_ja)\n",
    "\n",
    "# Save\n",
    "base_name = os.path.splitext(os.path.basename(AUDIO_PATH))[0]\n",
    "srt_ja_path = f\"/content/{base_name}_ja.srt\"\n",
    "with open(srt_ja_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_ja)\n",
    "\n",
    "print(f\"‚úÖ Japanese SRT saved to: {srt_ja_path}\")\n",
    "print(f\"   {len(subtitles_ja)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_ja.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602d2d1",
   "metadata": {},
   "source": [
    "## 5 ¬∑ Translate Subtitles to English\n",
    "\n",
    "Uses [Helsinki-NLP/opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) ‚Äî a lightweight MarianMT model specifically trained for Japanese ‚Üí English translation. It runs comfortably on Colab alongside the ASR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "TRANSLATION_MODEL = \"Helsinki-NLP/opus-mt-ja-en\"\n",
    "\n",
    "print(f\"Loading translation model: {TRANSLATION_MODEL} ‚Ä¶\")\n",
    "trans_tokenizer = MarianTokenizer.from_pretrained(TRANSLATION_MODEL)\n",
    "trans_model = MarianMTModel.from_pretrained(TRANSLATION_MODEL).to(\"cuda\")\n",
    "print(\"‚úÖ Translation model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(texts: list[str], batch_size: int = 32) -> list[str]:\n",
    "    \"\"\"Translate a list of Japanese texts to English in batches.\"\"\"\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = trans_tokenizer(\n",
    "            batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = trans_model.generate(**inputs, max_length=512)\n",
    "        decoded = trans_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "\n",
    "# Extract Japanese texts from subtitles\n",
    "ja_texts = [text for _, _, text in subtitles_ja]\n",
    "\n",
    "print(f\"Translating {len(ja_texts)} subtitle segments ‚Ä¶\")\n",
    "en_texts = translate_texts(ja_texts)\n",
    "print(\"‚úÖ Translation complete.\")\n",
    "\n",
    "# Build English subtitles with original timings\n",
    "subtitles_en = [\n",
    "    (start, end, en_text) for (start, end, _), en_text in zip(subtitles_ja, en_texts)\n",
    "]\n",
    "\n",
    "srt_en = build_srt(subtitles_en)\n",
    "\n",
    "# Save\n",
    "srt_en_path = f\"/content/{base_name}_en.srt\"\n",
    "with open(srt_en_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_en)\n",
    "\n",
    "print(f\"\\n‚úÖ English SRT saved to: {srt_en_path}\")\n",
    "print(f\"   {len(subtitles_en)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_en.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e65c2",
   "metadata": {},
   "source": [
    "## 6 ¬∑ Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e116ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'#':>3}  {'Time':^27}  {'Japanese':<40}  {'English':<40}\")\n",
    "print(\"‚îÄ\" * 115)\n",
    "for i, ((s, e, ja), (_, _, en)) in enumerate(zip(subtitles_ja, subtitles_en), 1):\n",
    "    time_str = f\"{format_srt_time(s)} ‚Üí {format_srt_time(e)}\"\n",
    "    print(f\"{i:>3}  {time_str}  {ja:<40}  {en:<40}\")\n",
    "    if i >= 30:\n",
    "        remaining = len(subtitles_ja) - 30\n",
    "        if remaining > 0:\n",
    "            print(f\"\\n... and {remaining} more segments.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0a5d8",
   "metadata": {},
   "source": [
    "## 7 ¬∑ Download SRT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    print(\"Downloading Japanese SRT ‚Ä¶\")\n",
    "    files.download(srt_ja_path)\n",
    "    print(\"Downloading English SRT ‚Ä¶\")\n",
    "    files.download(srt_en_path)\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab ‚Äî files saved at:\")\n",
    "    print(f\"  Japanese: {srt_ja_path}\")\n",
    "    print(f\"  English:  {srt_en_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321c21",
   "metadata": {},
   "source": [
    "## 8 ¬∑ Cleanup (Optional)\n",
    "\n",
    "Free GPU memory if you want to run other things in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del asr_model, trans_model, trans_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ GPU memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
