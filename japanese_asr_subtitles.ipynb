{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220543da",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Japanese Audio ‚Üí SRT Subtitles (JP + EN)\n",
    "\n",
    "This notebook:\n",
    "1. **Transcribes** uploaded Japanese audio using [Qwen/Qwen3-ASR-1.7B](https://huggingface.co/Qwen/Qwen3-ASR-1.7B) with word-level timestamps via the ForcedAligner\n",
    "2. **Generates** an SRT subtitle file from the transcription\n",
    "3. **Translates** the Japanese SRT to English using [Helsinki-NLP/opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en)\n",
    "\n",
    "**Requirements:** A Colab runtime with a **T4 GPU** (free tier works).\n",
    "\n",
    "> ‚ö†Ô∏è Make sure you've selected **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7cff5",
   "metadata": {},
   "source": [
    "## 1 ¬∑ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q qwen-asr transformers sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41672cc7",
   "metadata": {},
   "source": [
    "## 2 ¬∑ Upload Japanese Audio File\n",
    "\n",
    "Supported formats: `.wav`, `.mp3`, `.flac`, `.ogg`, `.m4a`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57abc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML\n",
    "import os\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept=\".wav,.mp3,.flac,.ogg,.m4a,.aac,.wma,.opus\",\n",
    "    multiple=False,\n",
    "    description=\"Select audio\",\n",
    "    button_style=\"primary\",\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "status = widgets.HTML(value=\"<i>No file selected.</i>\")\n",
    "\n",
    "AUDIO_PATH = None\n",
    "\n",
    "\n",
    "def on_upload(change):\n",
    "    global AUDIO_PATH\n",
    "    uploaded = change[\"new\"]\n",
    "    if uploaded:\n",
    "        file_info = uploaded[0]\n",
    "        name = file_info[\"name\"]\n",
    "        content = file_info[\"content\"]\n",
    "        AUDIO_PATH = os.path.join(\"/content\", name)\n",
    "        with open(AUDIO_PATH, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        size_mb = len(content) / (1024 * 1024)\n",
    "        status.value = f\"‚úÖ <b>{name}</b> uploaded ({size_mb:.1f} MB)\"\n",
    "\n",
    "\n",
    "uploader.observe(on_upload, names=\"value\")\n",
    "display(widgets.VBox([uploader, status]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd319ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Preview the uploaded audio\n",
    "\n",
    "AUDIO_PATH = \"ja_audio.mp3\"\n",
    "\n",
    "if AUDIO_PATH and os.path.exists(AUDIO_PATH):\n",
    "    display(Audio(AUDIO_PATH))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please upload an audio file in the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600d6d4",
   "metadata": {},
   "source": [
    "## 3 ¬∑ Transcribe with Qwen3-ASR-1.7B\n",
    "\n",
    "To fit on a T4 (15 GB VRAM), we run ASR and alignment as **two separate steps** so both models are never loaded at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83adc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration { display-mode: \"form\" }\n",
    "\n",
    "# @markdown **Chunk length (seconds)** ‚Äî Each audio chunk is processed separately\n",
    "# @markdown to fit in GPU memory. Shorter = less VRAM but more chunks.\n",
    "# @markdown 20 s works on a free-tier T4 (15 GB). Increase if you have more VRAM.\n",
    "CHUNK_SEC = 20  # @param {type:\"slider\", min:5, max:120, step:5}\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown **Gemini translation batch size** ‚Äî Number of subtitle lines sent\n",
    "# @markdown per API call. Larger = fewer calls but longer prompts.\n",
    "GEMINI_BATCH_SIZE = 100  # @param {type:\"slider\", min:10, max:500, step:10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from qwen_asr import Qwen3ASRModel\n",
    "\n",
    "# Help PyTorch reuse freed VRAM fragments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "assert AUDIO_PATH and os.path.exists(AUDIO_PATH), (\n",
    "    \"No audio file found. Run the upload cell above first.\"\n",
    ")\n",
    "\n",
    "# --- Load and split audio manually ---\n",
    "# The audio tower's attention is O(n¬≤) on sequence length, so we split\n",
    "# into short segments and feed each one individually.\n",
    "SR = 16_000  # qwen-asr expects 16 kHz\n",
    "\n",
    "print(f\"Loading audio: {os.path.basename(AUDIO_PATH)} ‚Ä¶\")\n",
    "full_wav, _ = librosa.load(AUDIO_PATH, sr=SR, mono=True)\n",
    "total_dur = len(full_wav) / SR\n",
    "print(f\"Duration: {total_dur:.1f} s  ({total_dur / 60:.1f} min)\")\n",
    "\n",
    "chunk_samples = CHUNK_SEC * SR\n",
    "audio_chunks = []\n",
    "for start in range(0, len(full_wav), chunk_samples):\n",
    "    chunk = full_wav[start : start + chunk_samples]\n",
    "    if len(chunk) < SR // 2:  # skip tiny tail < 0.5 s\n",
    "        continue\n",
    "    audio_chunks.append((float(start) / SR, chunk))\n",
    "\n",
    "print(f\"Split into {len(audio_chunks)} chunks of ‚â§{CHUNK_SEC} s each.\")\n",
    "\n",
    "# --- Load ASR model ---\n",
    "print(\"\\nLoading Qwen3-ASR-1.7B ‚Ä¶\")\n",
    "asr_model = Qwen3ASRModel.from_pretrained(\n",
    "    \"Qwen/Qwen3-ASR-1.7B\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    max_inference_batch_size=1,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "print(\"‚úÖ ASR model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe each chunk individually to stay within T4 VRAM\n",
    "all_texts = []\n",
    "for i, (offset, chunk_wav) in enumerate(audio_chunks):\n",
    "    print(\n",
    "        f\"  Chunk {i + 1}/{len(audio_chunks)}  \"\n",
    "        f\"[{offset:.1f}s ‚Äì {offset + len(chunk_wav) / SR:.1f}s] ‚Ä¶\",\n",
    "        end=\" \",\n",
    "    )\n",
    "    r = asr_model.transcribe(\n",
    "        audio=(chunk_wav, SR),\n",
    "        language=\"Japanese\",\n",
    "        return_time_stamps=False,\n",
    "    )\n",
    "    text = r[0].text.strip()\n",
    "    all_texts.append(text)\n",
    "    print(text[:80])\n",
    "\n",
    "transcribed_text = \"\".join(all_texts)\n",
    "print(f\"\\n{'‚îÄ' * 60}\")\n",
    "print(f\"Full transcription ({len(transcribed_text)} chars):\\n{transcribed_text}\")\n",
    "\n",
    "# Free ASR model before loading the aligner\n",
    "del asr_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ ASR model unloaded ‚Äî GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f994619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Forced Aligner for word-level timestamps ---\n",
    "from dataclasses import replace\n",
    "from qwen_asr import Qwen3ForcedAligner\n",
    "\n",
    "print(\"Loading Qwen3-ForcedAligner-0.6B ‚Ä¶\")\n",
    "aligner = Qwen3ForcedAligner.from_pretrained(\n",
    "    \"Qwen/Qwen3-ForcedAligner-0.6B\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "print(\"‚úÖ Aligner loaded.\")\n",
    "\n",
    "# Align each chunk separately (same chunking as ASR) and shift timestamps\n",
    "print(\"Aligning timestamps ‚Ä¶\")\n",
    "time_stamps = []\n",
    "for i, (offset, chunk_wav) in enumerate(audio_chunks):\n",
    "    chunk_text = all_texts[i]\n",
    "    if not chunk_text.strip():\n",
    "        continue\n",
    "    print(f\"  Aligning chunk {i + 1}/{len(audio_chunks)} ‚Ä¶\")\n",
    "    alignment = aligner.align(\n",
    "        audio=(chunk_wav, SR),\n",
    "        text=chunk_text,\n",
    "        language=\"Japanese\",\n",
    "    )\n",
    "    # Shift timestamps by the chunk's offset (stamps are frozen dataclasses)\n",
    "    for stamp in alignment[0]:\n",
    "        shifted = replace(\n",
    "            stamp,\n",
    "            start_time=stamp.start_time + offset,\n",
    "            end_time=stamp.end_time + offset,\n",
    "        )\n",
    "        time_stamps.append(shifted)\n",
    "\n",
    "print(f\"\\nTimestamp segments: {len(time_stamps)}\")\n",
    "if time_stamps:\n",
    "    print(\n",
    "        f\"First: {time_stamps[0].text} [{time_stamps[0].start_time:.2f}s ‚Äì {time_stamps[0].end_time:.2f}s]\"\n",
    "    )\n",
    "\n",
    "# Free aligner\n",
    "del aligner\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ Aligner unloaded ‚Äî GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3baa9",
   "metadata": {},
   "source": [
    "## 4 ¬∑ Generate SRT Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def format_srt_time(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to SRT timestamp format: HH:MM:SS,mmm\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    secs = total_seconds % 60\n",
    "    millis = int(td.microseconds / 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n",
    "\n",
    "\n",
    "def group_timestamps_to_subtitles(\n",
    "    stamps, max_chars: int = 40, max_duration: float = 7.0, gap_threshold: float = 0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Group word-level timestamps into subtitle segments.\n",
    "\n",
    "    Args:\n",
    "        stamps: list of timestamp objects with .text, .start_time, .end_time\n",
    "        max_chars: max characters per subtitle line\n",
    "        max_duration: max duration (seconds) per subtitle\n",
    "        gap_threshold: silence gap (seconds) that forces a new subtitle\n",
    "    \"\"\"\n",
    "    if not stamps:\n",
    "        return []\n",
    "\n",
    "    subtitles = []\n",
    "    current_text = \"\"\n",
    "    current_start = stamps[0].start_time\n",
    "    current_end = stamps[0].end_time\n",
    "\n",
    "    for i, stamp in enumerate(stamps):\n",
    "        # Decide whether to start a new subtitle\n",
    "        start_new = False\n",
    "        if i == 0:\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "            continue\n",
    "\n",
    "        # Check gap between previous and current word\n",
    "        gap = stamp.start_time - current_end\n",
    "        new_duration = stamp.end_time - current_start\n",
    "        new_len = len(current_text) + len(stamp.text)\n",
    "\n",
    "        if gap > gap_threshold or new_duration > max_duration or new_len > max_chars:\n",
    "            start_new = True\n",
    "\n",
    "        if start_new:\n",
    "            subtitles.append((current_start, current_end, current_text.strip()))\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "        else:\n",
    "            current_text += stamp.text\n",
    "            current_end = stamp.end_time\n",
    "\n",
    "    # Don't forget the last segment\n",
    "    if current_text.strip():\n",
    "        subtitles.append((current_start, current_end, current_text.strip()))\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def build_srt(subtitles) -> str:\n",
    "    \"\"\"Build SRT string from list of (start, end, text) tuples.\"\"\"\n",
    "    lines = []\n",
    "    for idx, (start, end, text) in enumerate(subtitles, 1):\n",
    "        lines.append(str(idx))\n",
    "        lines.append(f\"{format_srt_time(start)} --> {format_srt_time(end)}\")\n",
    "        lines.append(text)\n",
    "        lines.append(\"\")  # blank line separator\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Build Japanese SRT\n",
    "subtitles_ja = group_timestamps_to_subtitles(time_stamps)\n",
    "\n",
    "srt_ja = build_srt(subtitles_ja)\n",
    "\n",
    "# Save\n",
    "base_name = os.path.splitext(os.path.basename(AUDIO_PATH))[0]\n",
    "srt_ja_path = f\"/content/{base_name}_ja.srt\"\n",
    "with open(srt_ja_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_ja)\n",
    "\n",
    "print(f\"‚úÖ Japanese SRT saved to: {srt_ja_path}\")\n",
    "print(f\"   {len(subtitles_ja)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_ja.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602d2d1",
   "metadata": {},
   "source": [
    "## 5 ¬∑ Translate Subtitles to English (opus-mt, local)\n",
    "\n",
    "Uses [Helsinki-NLP/opus-mt-ja-en](https://huggingface.co/Helsinki-NLP/opus-mt-ja-en) ‚Äî a lightweight MarianMT model for Japanese ‚Üí English. Fast and runs entirely on-device, but quality is limited for nuanced text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "TRANSLATION_MODEL = \"Helsinki-NLP/opus-mt-ja-en\"\n",
    "\n",
    "print(f\"Loading translation model: {TRANSLATION_MODEL} ‚Ä¶\")\n",
    "trans_tokenizer = MarianTokenizer.from_pretrained(TRANSLATION_MODEL)\n",
    "trans_model = MarianMTModel.from_pretrained(TRANSLATION_MODEL).to(\"cuda\")\n",
    "print(\"‚úÖ Translation model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(texts: list[str], batch_size: int = 32) -> list[str]:\n",
    "    \"\"\"Translate a list of Japanese texts to English in batches.\"\"\"\n",
    "    translations = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = trans_tokenizer(\n",
    "            batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = trans_model.generate(**inputs, max_length=512)\n",
    "        decoded = trans_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "\n",
    "# Extract Japanese texts from subtitles\n",
    "ja_texts = [text for _, _, text in subtitles_ja]\n",
    "\n",
    "print(f\"Translating {len(ja_texts)} subtitle segments ‚Ä¶\")\n",
    "en_texts = translate_texts(ja_texts)\n",
    "print(\"‚úÖ Translation complete.\")\n",
    "\n",
    "# Build English subtitles with original timings\n",
    "subtitles_en = [\n",
    "    (start, end, en_text) for (start, end, _), en_text in zip(subtitles_ja, en_texts)\n",
    "]\n",
    "\n",
    "srt_en = build_srt(subtitles_en)\n",
    "\n",
    "# Save\n",
    "srt_en_path = f\"/content/{base_name}_en.srt\"\n",
    "with open(srt_en_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_en)\n",
    "\n",
    "print(f\"\\n‚úÖ English SRT saved to: {srt_en_path}\")\n",
    "print(f\"   {len(subtitles_en)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_en.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b583c1",
   "metadata": {},
   "source": [
    "## 6 ¬∑ Translate Subtitles with Gemini 3 Flash (free via Colab)\n",
    "\n",
    "Google Colab provides free access to Gemini models through the built-in `google.generativeai` API. Gemini 3 Flash produces much higher-quality translations than the small opus-mt model ‚Äî especially for nuanced or conversational Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8300e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Colab provides a free Gemini API key automatically\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    # Fallback: use the Colab-provided default\n",
    "    import os\n",
    "\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "assert api_key, (\n",
    "    \"No Gemini API key found. In Colab, go to üîë Secrets (left sidebar) \"\n",
    "    \"and add GOOGLE_API_KEY, or enable the free Gemini integration.\"\n",
    ")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-3-flash\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a professional Japanese-to-English subtitle translator. \"\n",
    "    \"You will receive numbered Japanese subtitle lines. \"\n",
    "    \"Return ONLY a JSON array of strings ‚Äî one English translation per line, \"\n",
    "    \"in the same order. Keep translations concise and natural for subtitles. \"\n",
    "    \"Preserve the original meaning and tone. Do NOT add numbering or extra text.\"\n",
    ")\n",
    "\n",
    "\n",
    "def translate_with_gemini(\n",
    "    texts: list[str], batch_size: int = GEMINI_BATCH_SIZE\n",
    ") -> list[str]:\n",
    "    \"\"\"Translate Japanese texts to English using Gemini 3 Flash in batches.\"\"\"\n",
    "    all_translations = []\n",
    "\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch = texts[batch_start : batch_start + batch_size]\n",
    "        batch_num = batch_start // batch_size + 1\n",
    "        total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "        print(f\"  Batch {batch_num}/{total_batches} ({len(batch)} lines) ‚Ä¶\", end=\" \")\n",
    "\n",
    "        # Build numbered input\n",
    "        numbered = \"\\n\".join(f\"{i + 1}. {t}\" for i, t in enumerate(batch))\n",
    "        prompt = f\"{SYSTEM_PROMPT}\\n\\nSubtitle lines:\\n{numbered}\"\n",
    "\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = gemini_model.generate_content(prompt)\n",
    "                raw = response.text.strip()\n",
    "                # Strip markdown code fences if present\n",
    "                if raw.startswith(\"```\"):\n",
    "                    raw = raw.split(\"\\n\", 1)[1]\n",
    "                    raw = raw.rsplit(\"```\", 1)[0]\n",
    "                translations = json.loads(raw)\n",
    "                assert isinstance(translations, list) and len(translations) == len(\n",
    "                    batch\n",
    "                )\n",
    "                all_translations.extend(translations)\n",
    "                print(\"‚úÖ\")\n",
    "                break\n",
    "            except (json.JSONDecodeError, AssertionError, Exception) as e:\n",
    "                if attempt < 2:\n",
    "                    print(f\"‚ö†Ô∏è retry ({e.__class__.__name__}) ‚Ä¶\", end=\" \")\n",
    "                    time.sleep(2**attempt)\n",
    "                else:\n",
    "                    # Fallback: return originals for this batch\n",
    "                    print(f\"‚ùå fallback (kept Japanese)\")\n",
    "                    all_translations.extend(batch)\n",
    "\n",
    "        # Respect free-tier rate limits\n",
    "        if batch_start + batch_size < len(texts):\n",
    "            time.sleep(1)\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "\n",
    "# --- Translate ---\n",
    "ja_texts_gemini = [text for _, _, text in subtitles_ja]\n",
    "print(f\"Translating {len(ja_texts_gemini)} subtitles with Gemini 3 Flash ‚Ä¶\\n\")\n",
    "en_texts_gemini = translate_with_gemini(ja_texts_gemini)\n",
    "print(f\"\\n‚úÖ Gemini translation complete.\")\n",
    "\n",
    "# Build English subtitles with original timings\n",
    "subtitles_en_gemini = [\n",
    "    (start, end, en_text)\n",
    "    for (start, end, _), en_text in zip(subtitles_ja, en_texts_gemini)\n",
    "]\n",
    "\n",
    "srt_en_gemini = build_srt(subtitles_en_gemini)\n",
    "\n",
    "# Save\n",
    "srt_en_gemini_path = f\"/content/{base_name}_en_gemini.srt\"\n",
    "with open(srt_en_gemini_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_en_gemini)\n",
    "\n",
    "print(f\"‚úÖ Gemini English SRT saved to: {srt_en_gemini_path}\")\n",
    "print(f\"   {len(subtitles_en_gemini)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_en_gemini.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e65c2",
   "metadata": {},
   "source": [
    "## 7 ¬∑ Side-by-Side Comparison (opus-mt vs Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e116ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = (\n",
    "    f\"{'#':>3}  {'Time':^27}  {'Japanese':<30}  {'opus-mt':<30}  {'Gemini 3 Flash':<30}\"\n",
    ")\n",
    "print(header)\n",
    "print(\"‚îÄ\" * len(header))\n",
    "for i, ((s, e, ja), (_, _, en_opus), (_, _, en_gem)) in enumerate(\n",
    "    zip(subtitles_ja, subtitles_en, subtitles_en_gemini), 1\n",
    "):\n",
    "    time_str = f\"{format_srt_time(s)} ‚Üí {format_srt_time(e)}\"\n",
    "    print(f\"{i:>3}  {time_str}  {ja:<30}  {en_opus:<30}  {en_gem:<30}\")\n",
    "    if i >= 30:\n",
    "        remaining = len(subtitles_ja) - 30\n",
    "        if remaining > 0:\n",
    "            print(f\"\\n... and {remaining} more segments.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0a5d8",
   "metadata": {},
   "source": [
    "## 8 ¬∑ Download SRT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    print(\"Downloading Japanese SRT ‚Ä¶\")\n",
    "    files.download(srt_ja_path)\n",
    "    print(\"Downloading English SRT (opus-mt) ‚Ä¶\")\n",
    "    files.download(srt_en_path)\n",
    "    print(\"Downloading English SRT (Gemini) ‚Ä¶\")\n",
    "    files.download(srt_en_gemini_path)\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab ‚Äî files saved at:\")\n",
    "    print(f\"  Japanese:        {srt_ja_path}\")\n",
    "    print(f\"  English (opus):  {srt_en_path}\")\n",
    "    print(f\"  English (Gemini): {srt_en_gemini_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321c21",
   "metadata": {},
   "source": [
    "## 9 ¬∑ Cleanup (Optional)\n",
    "\n",
    "Free GPU memory if you want to run other things in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del trans_model, trans_tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ GPU memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
