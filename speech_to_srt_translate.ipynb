{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220543da",
   "metadata": {
    "id": "220543da"
   },
   "source": [
    "# üéôÔ∏è Audio ‚Üí SRT Subtitles (with Translation)\n",
    "\n",
    "This notebook:\n",
    "1. **Transcribes** uploaded audio using [Qwen/Qwen3-ASR-1.7B](https://huggingface.co/Qwen/Qwen3-ASR-1.7B) with word-level timestamps via the ForcedAligner\n",
    "2. **Generates** an SRT subtitle file from the transcription\n",
    "3. **Translates** the SRT to a target language using [Helsinki-NLP/opus-mt](https://huggingface.co/Helsinki-NLP), Gemini or Google Translate\n",
    "\n",
    "Configure `SOURCE_LANGUAGE` and `TARGET_LANGUAGE` in the config cell below (default: Japanese ‚Üí English).\n",
    "\n",
    "**Requirements:** A Colab runtime with a **T4 GPU** (free tier works).\n",
    "\n",
    "> ‚ö†Ô∏è Make sure you've selected **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTe9ZS_JZbQ-",
   "metadata": {
    "id": "LTe9ZS_JZbQ-"
   },
   "source": [
    "## 0 ¬∑ CONFIG\n",
    "\n",
    "General Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh6XyYsVZbqi",
   "metadata": {
    "id": "Vh6XyYsVZbqi"
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT ‚ö†Ô∏è\n",
    "# Path to the file to transcribe ‚Äî Supported formats:\n",
    "# .wav, .mp3, .flac, .ogg, .m4a, etc.\n",
    "AUDIO_PATH = \"ja_audio.mp3\"  # @param {type:\"string\"}\n",
    "\n",
    "# Source and target languages for transcription and translation\n",
    "SOURCE_LANGUAGE = \"Japanese\"  # @param {type:\"string\"}\n",
    "TARGET_LANGUAGE = \"English\"  # @param {type:\"string\"}\n",
    "\n",
    "# Which translation methods to run\n",
    "TRANSLATE_USING_GEMINI = True  # Gemini (Using your API key)\n",
    "TRANSLATE_USING_GT = True  # Google Translate\n",
    "\n",
    "# Legacy: Translate using a small open source model\n",
    "TRANSLATE_USING_OPUS = False\n",
    "\n",
    "# ISO 639-1 language codes ‚Äî used for file naming and translation APIs.\n",
    "# Add more as needed.\n",
    "LANG_CODES = {\n",
    "    \"Arabic\": \"ar\",\n",
    "    \"Chinese\": \"zh\",\n",
    "    \"Czech\": \"cs\",\n",
    "    \"Danish\": \"da\",\n",
    "    \"Dutch\": \"nl\",\n",
    "    \"English\": \"en\",\n",
    "    \"Finnish\": \"fi\",\n",
    "    \"French\": \"fr\",\n",
    "    \"German\": \"de\",\n",
    "    \"Greek\": \"el\",\n",
    "    \"Hebrew\": \"he\",\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"Hungarian\": \"hu\",\n",
    "    \"Indonesian\": \"id\",\n",
    "    \"Italian\": \"it\",\n",
    "    \"Japanese\": \"ja\",\n",
    "    \"Korean\": \"ko\",\n",
    "    \"Malay\": \"ms\",\n",
    "    \"Norwegian\": \"no\",\n",
    "    \"Polish\": \"pl\",\n",
    "    \"Portuguese\": \"pt\",\n",
    "    \"Romanian\": \"ro\",\n",
    "    \"Russian\": \"ru\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"Swedish\": \"sv\",\n",
    "    \"Thai\": \"th\",\n",
    "    \"Turkish\": \"tr\",\n",
    "    \"Ukrainian\": \"uk\",\n",
    "    \"Vietnamese\": \"vi\",\n",
    "}\n",
    "SRC_CODE = LANG_CODES[SOURCE_LANGUAGE]\n",
    "TGT_CODE = LANG_CODES[TARGET_LANGUAGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kz5ddNOpfVrv",
   "metadata": {
    "id": "kz5ddNOpfVrv"
   },
   "source": [
    "Technical Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WbhfwMkTfW0h",
   "metadata": {
    "id": "WbhfwMkTfW0h"
   },
   "outputs": [],
   "source": [
    "# Chunk length (seconds) ‚Äî Each audio chunk is processed separately\n",
    "# to fit in GPU memory. Shorter = less VRAM but more chunks.\n",
    "# 20 s works on a free-tier T4 (15 GB). Increase if possible.\n",
    "CHUNK_SEC = 200\n",
    "\n",
    "# Maximum batch size for the ASR Model\n",
    "MAX_INFERENCE_BATCH_SIZE = 32  # TEST IF 32 WORKS, CHANGE TO 1 AGAIN IF NOT\n",
    "\n",
    "# Gemini translation batch size ‚Äî Number of subtitle lines sent\n",
    "# per API call. Larger = fewer calls but longer prompts.\n",
    "GEMINI_BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7cff5",
   "metadata": {
    "id": "b1c7cff5"
   },
   "source": [
    "## 1 ¬∑ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933680e",
   "metadata": {
    "id": "9933680e"
   },
   "outputs": [],
   "source": [
    "%pip install -q qwen-asr transformers sentencepiece sacremoses deep-translator google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KTlr4jWHtWmj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTlr4jWHtWmj",
    "outputId": "3806f40b-d6f4-4abf-ebf1-f7011c4eda24"
   },
   "outputs": [],
   "source": [
    "%pip install -U -q https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3%2Bcu128torch2.10-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41672cc7",
   "metadata": {
    "id": "41672cc7"
   },
   "source": [
    "## 2 ¬∑ Upload Audio File\n",
    "\n",
    "Supported formats: `.wav`, `.mp3`, `.flac`, `.ogg`, `.m4a`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd319ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "3bd319ef",
    "outputId": "c3703591-3f88-418e-b54e-e44732cb4e75"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "import os\n",
    "\n",
    "# Preview the uploaded audio\n",
    "if AUDIO_PATH and os.path.exists(AUDIO_PATH):\n",
    "    display(Audio(AUDIO_PATH))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please upload an audio file in the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600d6d4",
   "metadata": {
    "id": "7600d6d4"
   },
   "source": [
    "## 3 ¬∑ Transcribe with Qwen3-ASR-1.7B\n",
    "\n",
    "To fit on a T4 (15 GB VRAM), we run ASR and alignment as **two separate steps** so both models are never loaded at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bc4f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "d55e7b10d1544ecc8503cdf363732622",
      "9c0c0f539341452a9ea60137b28cb249",
      "92b941db825d40a5a88c8954cff53dbb",
      "aa7a8cfe307c413b92607e4cf1754ffe",
      "030fbd75958d40f3bfb6986b632ca4f3",
      "2edae9b923f545c2ae09464fa18dd73f",
      "aaecbd66f31246ddb27ebf19d50694a2",
      "deb1e97276664d26a647a4c2681cac99",
      "b142015ef28340f9a7b6b0c571cd9297",
      "44de2f6aa35f49109a55e4ad78d52268",
      "b82a844a83e94c3db3e37d2583edd546"
     ]
    },
    "id": "816bc4f7",
    "outputId": "23a2224f-2eb9-45d1-dad1-8c8ef11fa4ad"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from qwen_asr import Qwen3ASRModel\n",
    "\n",
    "# Help PyTorch reuse freed VRAM fragments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "assert AUDIO_PATH and os.path.exists(AUDIO_PATH), (\n",
    "    \"No audio file found. Run the upload cell above first.\"\n",
    ")\n",
    "\n",
    "# --- Load and split audio manually ---\n",
    "# The audio tower's attention is O(n¬≤) on sequence length, so we split\n",
    "# into short segments and feed each one individually.\n",
    "SR = 16_000  # qwen-asr expects 16 kHz\n",
    "\n",
    "print(f\"Loading audio: {os.path.basename(AUDIO_PATH)} ‚Ä¶\")\n",
    "full_wav, _ = librosa.load(AUDIO_PATH, sr=SR, mono=True)\n",
    "total_dur = len(full_wav) / SR\n",
    "print(f\"Duration: {total_dur:.1f} s  ({total_dur / 60:.1f} min)\")\n",
    "\n",
    "chunk_samples = CHUNK_SEC * SR\n",
    "audio_chunks = []\n",
    "for start in range(0, len(full_wav), chunk_samples):\n",
    "    chunk = full_wav[start : start + chunk_samples]\n",
    "    if len(chunk) < SR // 2:  # skip tiny tail < 0.5 s\n",
    "        continue\n",
    "    audio_chunks.append((float(start) / SR, chunk))\n",
    "\n",
    "print(f\"Split into {len(audio_chunks)} chunks of ‚â§{CHUNK_SEC} s each.\")\n",
    "\n",
    "# --- Load ASR model ---\n",
    "print(\"\\nLoading Qwen3-ASR-1.7B ‚Ä¶\")\n",
    "asr_model = Qwen3ASRModel.from_pretrained(\n",
    "    \"Qwen/Qwen3-ASR-1.7B\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    max_inference_batch_size=MAX_INFERENCE_BATCH_SIZE,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "print(\"‚úÖ ASR model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45c3e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a45c3e5",
    "outputId": "158e06ba-089c-437f-8504-1a63cf77af31"
   },
   "outputs": [],
   "source": [
    "# Transcribe each chunk individually to stay within T4 VRAM\n",
    "all_texts = []\n",
    "for i, (offset, chunk_wav) in enumerate(audio_chunks):\n",
    "    print(\n",
    "        f\"  Chunk {i + 1}/{len(audio_chunks)}  \"\n",
    "        f\"[{offset:.1f}s ‚Äì {offset + len(chunk_wav) / SR:.1f}s] ‚Ä¶\",\n",
    "        end=\" \",\n",
    "    )\n",
    "    r = asr_model.transcribe(\n",
    "        audio=(chunk_wav, SR),\n",
    "        language=SOURCE_LANGUAGE,\n",
    "        return_time_stamps=False,\n",
    "    )\n",
    "    text = r[0].text.strip()\n",
    "    all_texts.append(text)\n",
    "    print(text[:80])\n",
    "\n",
    "transcribed_text = \"\".join(all_texts)\n",
    "print(f\"\\n{'‚îÄ' * 60}\")\n",
    "print(f\"Full transcription ({len(transcribed_text)} chars):\\n{transcribed_text}\")\n",
    "\n",
    "# Free ASR model before loading the aligner\n",
    "del asr_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ ASR model unloaded ‚Äî GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjC-KHE52MCg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjC-KHE52MCg",
    "outputId": "6579dbbe-3d21-45a1-be95-f4d1936bc183"
   },
   "outputs": [],
   "source": [
    "# --- Step 2: Forced Aligner for word-level timestamps ---\n",
    "from dataclasses import replace\n",
    "from qwen_asr import Qwen3ForcedAligner\n",
    "\n",
    "print(\"Loading Qwen3-ForcedAligner-0.6B ‚Ä¶\")\n",
    "aligner = Qwen3ForcedAligner.from_pretrained(\n",
    "    \"Qwen/Qwen3-ForcedAligner-0.6B\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "print(\"‚úÖ Aligner loaded.\")\n",
    "\n",
    "# Align each chunk separately (same chunking as ASR) and shift timestamps\n",
    "print(\"Aligning timestamps ‚Ä¶\")\n",
    "time_stamps = []\n",
    "for i, (offset, chunk_wav) in enumerate(audio_chunks):\n",
    "    chunk_text = all_texts[i]\n",
    "    if not chunk_text.strip():\n",
    "        continue\n",
    "    print(f\"  Aligning chunk {i + 1}/{len(audio_chunks)} ‚Ä¶\")\n",
    "    alignment = aligner.align(\n",
    "        audio=(chunk_wav, SR),\n",
    "        text=chunk_text,\n",
    "        language=SOURCE_LANGUAGE,\n",
    "    )\n",
    "    # Shift timestamps by the chunk's offset (stamps are frozen dataclasses)\n",
    "    for stamp in alignment[0]:\n",
    "        shifted = replace(\n",
    "            stamp,\n",
    "            start_time=stamp.start_time + offset,\n",
    "            end_time=stamp.end_time + offset,\n",
    "        )\n",
    "        time_stamps.append(shifted)\n",
    "\n",
    "print(f\"\\nTimestamp segments: {len(time_stamps)}\")\n",
    "if time_stamps:\n",
    "    print(\n",
    "        f\"First: {time_stamps[0].text} [{time_stamps[0].start_time:.2f}s ‚Äì {time_stamps[0].end_time:.2f}s]\"\n",
    "    )\n",
    "\n",
    "# Free aligner\n",
    "del aligner\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úÖ Aligner unloaded ‚Äî GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3baa9",
   "metadata": {
    "id": "c0a3baa9"
   },
   "source": [
    "## 4 ¬∑ Generate SRT Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5731a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71c5731a",
    "outputId": "ef4ace6c-a53c-4775-8eb1-7ed12deb232d"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def format_srt_time(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to SRT timestamp format: HH:MM:SS,mmm\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    secs = total_seconds % 60\n",
    "    millis = int(td.microseconds / 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\"\n",
    "\n",
    "\n",
    "def group_timestamps_to_subtitles(\n",
    "    stamps, max_chars: int = 40, max_duration: float = 7.0, gap_threshold: float = 0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Group word-level timestamps into subtitle segments.\n",
    "\n",
    "    Args:\n",
    "        stamps: list of timestamp objects with .text, .start_time, .end_time\n",
    "        max_chars: max characters per subtitle line\n",
    "        max_duration: max duration (seconds) per subtitle\n",
    "        gap_threshold: silence gap (seconds) that forces a new subtitle\n",
    "    \"\"\"\n",
    "    if not stamps:\n",
    "        return []\n",
    "\n",
    "    subtitles = []\n",
    "    current_text = \"\"\n",
    "    current_start = stamps[0].start_time\n",
    "    current_end = stamps[0].end_time\n",
    "\n",
    "    for i, stamp in enumerate(stamps):\n",
    "        # Decide whether to start a new subtitle\n",
    "        start_new = False\n",
    "        if i == 0:\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "            continue\n",
    "\n",
    "        # Check gap between previous and current word\n",
    "        gap = stamp.start_time - current_end\n",
    "        new_duration = stamp.end_time - current_start\n",
    "        new_len = len(current_text) + len(stamp.text)\n",
    "\n",
    "        if gap > gap_threshold or new_duration > max_duration or new_len > max_chars:\n",
    "            start_new = True\n",
    "\n",
    "        if start_new:\n",
    "            subtitles.append((current_start, current_end, current_text.strip()))\n",
    "            current_text = stamp.text\n",
    "            current_start = stamp.start_time\n",
    "            current_end = stamp.end_time\n",
    "        else:\n",
    "            current_text += stamp.text\n",
    "            current_end = stamp.end_time\n",
    "\n",
    "    # Don't forget the last segment\n",
    "    if current_text.strip():\n",
    "        subtitles.append((current_start, current_end, current_text.strip()))\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def build_srt(subtitles) -> str:\n",
    "    \"\"\"Build SRT string from list of (start, end, text) tuples.\"\"\"\n",
    "    lines = []\n",
    "    for idx, (start, end, text) in enumerate(subtitles, 1):\n",
    "        lines.append(str(idx))\n",
    "        lines.append(f\"{format_srt_time(start)} --> {format_srt_time(end)}\")\n",
    "        lines.append(text)\n",
    "        lines.append(\"\")  # blank line separator\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Build source-language SRT\n",
    "subtitles_src = group_timestamps_to_subtitles(time_stamps)\n",
    "\n",
    "srt_src = build_srt(subtitles_src)\n",
    "\n",
    "# Save\n",
    "base_name = os.path.splitext(os.path.basename(AUDIO_PATH))[0]\n",
    "srt_src_path = f\"/content/{base_name}_{SRC_CODE}.srt\"\n",
    "with open(srt_src_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(srt_src)\n",
    "\n",
    "print(f\"‚úÖ {SOURCE_LANGUAGE} SRT saved to: {srt_src_path}\")\n",
    "print(f\"   {len(subtitles_src)} subtitle segments\\n\")\n",
    "print(\"--- Preview (first 10 segments) ---\")\n",
    "print(\"\\n\".join(srt_src.split(\"\\n\")[:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602d2d1",
   "metadata": {
    "id": "7602d2d1"
   },
   "source": [
    "## 5 ¬∑ Translate Subtitles (opus-mt, local)\n",
    "\n",
    "Uses a lightweight MarianMT model from [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) for translation. Fast and runs entirely on-device, but quality is limited for nuanced text. Honestly, this kinda sucks.\n",
    "\n",
    "> Model is auto-selected as `Helsinki-NLP/opus-mt-{SRC}-{TGT}`. Not all language pairs are available ‚Äî check [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) if you get errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ed59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9835ed59",
    "outputId": "e98969ca-4f61-4099-9958-b14c3694b8df"
   },
   "outputs": [],
   "source": [
    "if TRANSLATE_USING_OPUS:\n",
    "    from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "    TRANSLATION_MODEL = f\"Helsinki-NLP/opus-mt-{SRC_CODE}-{TGT_CODE}\"\n",
    "\n",
    "    print(f\"Loading translation model: {TRANSLATION_MODEL} ‚Ä¶\")\n",
    "    trans_tokenizer = MarianTokenizer.from_pretrained(TRANSLATION_MODEL)\n",
    "    trans_model = MarianMTModel.from_pretrained(TRANSLATION_MODEL).to(\"cuda\")\n",
    "    print(\"‚úÖ Translation model loaded.\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  opus-mt translation skipped (TRANSLATE_USING_OPUS = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35fdc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f35fdc1",
    "outputId": "b97e245d-39b8-4677-a94d-011f15469398"
   },
   "outputs": [],
   "source": [
    "subtitles_tgt = []\n",
    "srt_tgt_opus_path = None\n",
    "\n",
    "if TRANSLATE_USING_OPUS:\n",
    "\n",
    "    def translate_texts(texts: list[str], batch_size: int = 32) -> list[str]:\n",
    "        \"\"\"Translate a list of texts in batches using opus-mt.\"\"\"\n",
    "        translations = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = trans_tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            ).to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                output_ids = trans_model.generate(**inputs, max_length=512)\n",
    "            decoded = trans_tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            translations.extend(decoded)\n",
    "        return translations\n",
    "\n",
    "    # Extract source-language texts from subtitles\n",
    "    src_texts = [text for _, _, text in subtitles_src]\n",
    "    print(f\"Translating {len(src_texts)} subtitle segments ‚Ä¶\")\n",
    "    tgt_texts = translate_texts(src_texts)\n",
    "    print(\"‚úÖ Translation complete.\")\n",
    "\n",
    "    # Build target-language subtitles with original timings\n",
    "    subtitles_tgt = [\n",
    "        (start, end, tgt_text)\n",
    "        for (start, end, _), tgt_text in zip(subtitles_src, tgt_texts)\n",
    "    ]\n",
    "\n",
    "    srt_tgt = build_srt(subtitles_tgt)\n",
    "\n",
    "    # Save\n",
    "    srt_tgt_opus_path = f\"/content/{base_name}_{TGT_CODE}_opus.srt\"\n",
    "    with open(srt_tgt_opus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(srt_tgt)\n",
    "\n",
    "    print(f\"\\n‚úÖ {TARGET_LANGUAGE} SRT (opus-mt) saved to: {srt_tgt_opus_path}\")\n",
    "    print(f\"   {len(subtitles_tgt)} subtitle segments\\n\")\n",
    "    print(\"--- Preview (first 10 segments) ---\")\n",
    "    print(\"\\n\".join(srt_tgt.split(\"\\n\")[:40]))\n",
    "\n",
    "    # Free opus-mt model\n",
    "    del trans_model, trans_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  opus-mt translation skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QL3atzcfRbA1",
   "metadata": {
    "id": "QL3atzcfRbA1"
   },
   "source": [
    "## 6 ¬∑ Translate Subtitles with Gemini (free via Google AI Studio)\n",
    "\n",
    "Get an API key at https://aistudio.google.com/app/api-keys. By default, it uses\n",
    "a free tier, where billing isn't set up, so you shouldn't worry about getting\n",
    "charged. You can check the rate limits at https://aistudio.google.com/rate-limit?timeRange=last-28-days, and change the model accordingly by setting `GEMINI_MODEL` in the cell below.\n",
    "\n",
    "As of now, Gemini 3 Flash produces the highest quality translations (and are much higher-quality than the small opus-mt model ‚Äî especially for nuanced or conversational text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WQ6I9cTjRdm6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WQ6I9cTjRdm6",
    "outputId": "609c78b6-80c3-43aa-aaff-a3c715975e95"
   },
   "outputs": [],
   "source": [
    "subtitles_tgt_gemini = []\n",
    "srt_tgt_gemini_path = None\n",
    "\n",
    "if TRANSLATE_USING_GEMINI:\n",
    "    from google import genai\n",
    "    import json\n",
    "    import time\n",
    "\n",
    "    from google.colab import userdata\n",
    "\n",
    "    try:\n",
    "        api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    except userdata.SecretNotFoundError:\n",
    "        import os\n",
    "\n",
    "        api_key = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "    assert api_key, (\n",
    "        \"No Gemini API key found. In Colab, go to üîë Secrets (left sidebar) \"\n",
    "        \"and add GOOGLE_API_KEY.\"\n",
    "    )\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    GEMINI_MODEL = \"gemini-3-flash-preview\"\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        f\"You are a professional {SOURCE_LANGUAGE}-to-{TARGET_LANGUAGE} subtitle translator. \"\n",
    "        f\"You will receive numbered {SOURCE_LANGUAGE} subtitle lines. \"\n",
    "        f\"Return ONLY a JSON array of strings ‚Äî one {TARGET_LANGUAGE} translation per line, \"\n",
    "        \"in the same order. Follow professional subtitle standards:\\n\"\n",
    "        \"- Write natural, concise translations optimized for on-screen reading.\\n\"\n",
    "        \"- Preserve the original meaning, tone, and intent.\\n\"\n",
    "        \"- DO NOT use quotation marks for spoken dialogue.\\n\"\n",
    "        \"- If a line contains multiple speakers, separate them with a leading dash:\\n\"\n",
    "        \"- Keep punctuation simple and subtitle-appropriate.\\n\"\n",
    "        \"- Avoid unnecessary words, filler, or formal phrasing.\\n\"\n",
    "        \"- Maintain line readability and timing (short and clear).\\n\"\n",
    "        \"- Keep sound effects or non-speech text concise and natural.\\n\"\n",
    "        \"- Preserve speaker intent (emotion, politeness level, formality) when relevant.\\n\\n\"\n",
    "        \"Formatting rules:\\n\"\n",
    "        \"- Output ONLY a valid JSON array of strings.\\n\"\n",
    "        \"- No numbering.\\n\"\n",
    "        \"- No extra explanations or comments.\\n\"\n",
    "        \"- No additional quotation marks around dialogue.\\n\"\n",
    "        \"- Do not merge or split lines unless absolutely necessary for clarity.\\n\\n\"\n",
    "        \"Your translations should look like professional streaming subtitles, not written prose.\"\n",
    "    )\n",
    "\n",
    "    def translate_with_gemini(\n",
    "        texts: list[str], batch_size: int = GEMINI_BATCH_SIZE\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Translate texts using Gemini in batches.\"\"\"\n",
    "        all_translations = []\n",
    "\n",
    "        for batch_start in range(0, len(texts), batch_size):\n",
    "            batch = texts[batch_start : batch_start + batch_size]\n",
    "            batch_num = batch_start // batch_size + 1\n",
    "            total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "            print(\n",
    "                f\"  Batch {batch_num}/{total_batches} ({len(batch)} lines) ‚Ä¶\", end=\" \"\n",
    "            )\n",
    "\n",
    "            # Build numbered input\n",
    "            numbered = \"\\n\".join(f\"{i + 1}. {t}\" for i, t in enumerate(batch))\n",
    "            prompt = f\"{SYSTEM_PROMPT}\\n\\nSubtitle lines:\\n{numbered}\"\n",
    "\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    response = client.models.generate_content(\n",
    "                        model=GEMINI_MODEL,\n",
    "                        contents=prompt,\n",
    "                    )\n",
    "                    raw = response.text.strip()\n",
    "                    # Strip markdown code fences if present\n",
    "                    if raw.startswith(\"```\"):\n",
    "                        raw = raw.split(\"\\n\", 1)[1]\n",
    "                        raw = raw.rsplit(\"```\", 1)[0]\n",
    "                    translations = json.loads(raw)\n",
    "                    assert isinstance(translations, list) and len(translations) == len(\n",
    "                        batch\n",
    "                    )\n",
    "                    all_translations.extend(translations)\n",
    "                    print(\"‚úÖ\")\n",
    "                    break\n",
    "                except (json.JSONDecodeError, AssertionError, Exception) as e:\n",
    "                    if attempt < 2:\n",
    "                        print(f\"‚ö†Ô∏è retry ({e.__class__.__name__}) ‚Ä¶\", end=\" \")\n",
    "                        time.sleep(2**attempt)\n",
    "                    else:\n",
    "                        # Fallback: return originals for this batch\n",
    "                        print(f\"‚ùå fallback (kept {SOURCE_LANGUAGE})\")\n",
    "                        all_translations.extend(batch)\n",
    "\n",
    "            # Respect free-tier rate limits\n",
    "            if batch_start + batch_size < len(texts):\n",
    "                time.sleep(1)\n",
    "\n",
    "        return all_translations\n",
    "\n",
    "    # --- Translate ---\n",
    "    src_texts_gemini = [text for _, _, text in subtitles_src]\n",
    "    print(\n",
    "        f\"Translating {len(src_texts_gemini)} subtitles with Gemini ({GEMINI_MODEL}) ‚Ä¶\\n\"\n",
    "    )\n",
    "    tgt_texts_gemini = translate_with_gemini(src_texts_gemini)\n",
    "    print(\"\\n‚úÖ Gemini translation complete.\")\n",
    "\n",
    "    # Build target-language subtitles with original timings\n",
    "    subtitles_tgt_gemini = [\n",
    "        (start, end, tgt_text)\n",
    "        for (start, end, _), tgt_text in zip(subtitles_src, tgt_texts_gemini)\n",
    "    ]\n",
    "\n",
    "    srt_tgt_gemini = build_srt(subtitles_tgt_gemini)\n",
    "\n",
    "    # Save\n",
    "    srt_tgt_gemini_path = f\"/content/{base_name}_{TGT_CODE}_gemini.srt\"\n",
    "    with open(srt_tgt_gemini_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(srt_tgt_gemini)\n",
    "\n",
    "    print(f\"‚úÖ Gemini {TARGET_LANGUAGE} SRT saved to: {srt_tgt_gemini_path}\")\n",
    "    print(f\"   {len(subtitles_tgt_gemini)} subtitle segments\\n\")\n",
    "    print(\"--- Preview (first 10 segments) ---\")\n",
    "    print(\"\\n\".join(srt_tgt_gemini.split(\"\\n\")[:40]))\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Gemini translation skipped (TRANSLATE_USING_GEMINI = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665be06a",
   "metadata": {},
   "source": [
    "## 7 ¬∑ Translate Subtitles with Google Translate (free)\n",
    "\n",
    "Uses the free Google Translate API via `deep-translator`. No API key needed. Quality sits between opus-mt and Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_tgt_gt = []\n",
    "srt_tgt_gt_path = None\n",
    "\n",
    "if TRANSLATE_USING_GT:\n",
    "    from deep_translator import GoogleTranslator\n",
    "    import time\n",
    "\n",
    "    gtranslator = GoogleTranslator(source=SRC_CODE, target=TGT_CODE)\n",
    "\n",
    "    def translate_with_google(texts: list[str], batch_size: int = 50) -> list[str]:\n",
    "        \"\"\"Translate texts using Google Translate (free).\n",
    "\n",
    "        deep-translator's GoogleTranslator.translate_batch() has a ~5000 char\n",
    "        limit per request, so we send in small batches.\n",
    "        \"\"\"\n",
    "        all_translations = []\n",
    "\n",
    "        for batch_start in range(0, len(texts), batch_size):\n",
    "            batch = texts[batch_start : batch_start + batch_size]\n",
    "            batch_num = batch_start // batch_size + 1\n",
    "            total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "            print(\n",
    "                f\"  Batch {batch_num}/{total_batches} ({len(batch)} lines) ‚Ä¶\", end=\" \"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                translated = gtranslator.translate_batch(batch)\n",
    "                all_translations.extend(translated)\n",
    "                print(\"‚úÖ\")\n",
    "            except Exception as e:\n",
    "                # Fallback: translate one-by-one for this batch\n",
    "                print(f\"‚ö†Ô∏è batch failed ({e.__class__.__name__}), trying one-by-one ‚Ä¶\")\n",
    "                for t in batch:\n",
    "                    try:\n",
    "                        all_translations.append(gtranslator.translate(t))\n",
    "                    except Exception:\n",
    "                        all_translations.append(t)\n",
    "                    time.sleep(0.3)\n",
    "\n",
    "            # Small delay to avoid rate-limiting\n",
    "            if batch_start + batch_size < len(texts):\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        return all_translations\n",
    "\n",
    "    # --- Translate ---\n",
    "    src_texts_gt = [text for _, _, text in subtitles_src]\n",
    "    print(f\"Translating {len(src_texts_gt)} subtitles with Google Translate ‚Ä¶\\n\")\n",
    "    tgt_texts_gt = translate_with_google(src_texts_gt)\n",
    "    print(\"\\n‚úÖ Google Translate complete.\")\n",
    "\n",
    "    # Build target-language subtitles with original timings\n",
    "    subtitles_tgt_gt = [\n",
    "        (start, end, tgt_text)\n",
    "        for (start, end, _), tgt_text in zip(subtitles_src, tgt_texts_gt)\n",
    "    ]\n",
    "\n",
    "    srt_tgt_gt = build_srt(subtitles_tgt_gt)\n",
    "\n",
    "    # Save\n",
    "    srt_tgt_gt_path = f\"/content/{base_name}_{TGT_CODE}_gtranslate.srt\"\n",
    "    with open(srt_tgt_gt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(srt_tgt_gt)\n",
    "\n",
    "    print(f\"‚úÖ Google Translate SRT saved to: {srt_tgt_gt_path}\")\n",
    "    print(f\"   {len(subtitles_tgt_gt)} subtitle segments\\n\")\n",
    "    print(\"--- Preview (first 10 segments) ---\")\n",
    "    print(\"\\n\".join(srt_tgt_gt.split(\"\\n\")[:40]))\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Google Translate skipped (TRANSLATE_USING_GT = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e65c2",
   "metadata": {
    "id": "130e65c2"
   },
   "source": [
    "## 8 ¬∑ Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e116ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34e116ee",
    "outputId": "b1db1bf7-71e9-417e-afce-8da442c744ea"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML as IPyHTML\n",
    "\n",
    "# Build dynamic columns based on which translations ran\n",
    "cols = [(SOURCE_LANGUAGE, subtitles_src)]\n",
    "if subtitles_tgt:\n",
    "    cols.append((\"opus-mt\", subtitles_tgt))\n",
    "if subtitles_tgt_gemini:\n",
    "    cols.append((\"Gemini 3 Flash\", subtitles_tgt_gemini))\n",
    "if subtitles_tgt_gt:\n",
    "    cols.append((\"Google Translate\", subtitles_tgt_gt))\n",
    "\n",
    "ncols = 2 + len(cols)  # #, Time, + translation cols\n",
    "header_cells = \"\".join(f\"<th>{name}</th>\" for name, _ in cols)\n",
    "header = f\"<tr><th>#</th><th>Time</th>{header_cells}</tr>\"\n",
    "\n",
    "rows = []\n",
    "for i, (s, e, src) in enumerate(subtitles_src):\n",
    "    time_str = f\"{format_srt_time(s)} ‚Üí {format_srt_time(e)}\"\n",
    "    data_cells = \"\"\n",
    "    for _, subs in cols:\n",
    "        text = subs[i][2] if i < len(subs) else \"\"\n",
    "        data_cells += f\"<td>{text}</td>\"\n",
    "    rows.append(\n",
    "        f\"<tr><td>{i + 1}</td><td style='white-space:nowrap'>{time_str}</td>{data_cells}</tr>\"\n",
    "    )\n",
    "    if i >= 29:\n",
    "        remaining = len(subtitles_src) - 30\n",
    "        if remaining > 0:\n",
    "            rows.append(\n",
    "                f\"<tr><td colspan='{ncols}'><i>‚Ä¶ and {remaining} more segments</i></td></tr>\"\n",
    "            )\n",
    "        break\n",
    "\n",
    "html = (\n",
    "    \"<table border='1' cellpadding='4' style='border-collapse:collapse;font-size:13px'>\"\n",
    "    + header\n",
    "    + \"\\n\".join(rows)\n",
    "    + \"</table>\"\n",
    ")\n",
    "display(IPyHTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0a5d8",
   "metadata": {
    "id": "eef0a5d8"
   },
   "source": [
    "## 9 ¬∑ Download SRT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efb76b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "59efb76b",
    "outputId": "6ceed3f4-9372-411d-e1f4-ec2ca6b0ccec"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    print(f\"Downloading {SOURCE_LANGUAGE} SRT ‚Ä¶\")\n",
    "    files.download(srt_src_path)\n",
    "    if srt_tgt_opus_path:\n",
    "        print(f\"Downloading {TARGET_LANGUAGE} SRT (opus-mt) ‚Ä¶\")\n",
    "        files.download(srt_tgt_opus_path)\n",
    "    if srt_tgt_gemini_path:\n",
    "        print(f\"Downloading {TARGET_LANGUAGE} SRT (Gemini) ‚Ä¶\")\n",
    "        files.download(srt_tgt_gemini_path)\n",
    "    if srt_tgt_gt_path:\n",
    "        print(f\"Downloading {TARGET_LANGUAGE} SRT (Google Translate) ‚Ä¶\")\n",
    "        files.download(srt_tgt_gt_path)\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab ‚Äî files saved at:\")\n",
    "    print(f\"  {SOURCE_LANGUAGE}: {srt_src_path}\")\n",
    "    if srt_tgt_opus_path:\n",
    "        print(f\"  {TARGET_LANGUAGE} (opus-mt): {srt_tgt_opus_path}\")\n",
    "    if srt_tgt_gemini_path:\n",
    "        print(f\"  {TARGET_LANGUAGE} (Gemini): {srt_tgt_gemini_path}\")\n",
    "    if srt_tgt_gt_path:\n",
    "        print(f\"  {TARGET_LANGUAGE} (Google Trans.): {srt_tgt_gt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea321c21",
   "metadata": {
    "id": "ea321c21"
   },
   "source": [
    "## 10 ¬∑ Cleanup (Optional)\n",
    "\n",
    "Free GPU memory if you want to run other things in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3bf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "3b5a3bf8",
    "outputId": "76ce00e3-6464-4931-ae5f-49ad8e862107"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ GPU memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "030fbd75958d40f3bfb6986b632ca4f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2edae9b923f545c2ae09464fa18dd73f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44de2f6aa35f49109a55e4ad78d52268": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92b941db825d40a5a88c8954cff53dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_deb1e97276664d26a647a4c2681cac99",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b142015ef28340f9a7b6b0c571cd9297",
      "value": 2
     }
    },
    "9c0c0f539341452a9ea60137b28cb249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2edae9b923f545c2ae09464fa18dd73f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_aaecbd66f31246ddb27ebf19d50694a2",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "aa7a8cfe307c413b92607e4cf1754ffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44de2f6aa35f49109a55e4ad78d52268",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b82a844a83e94c3db3e37d2583edd546",
      "value": "‚Äá2/2‚Äá[00:18&lt;00:00,‚Äá‚Äá7.89s/it]"
     }
    },
    "aaecbd66f31246ddb27ebf19d50694a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b142015ef28340f9a7b6b0c571cd9297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b82a844a83e94c3db3e37d2583edd546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d55e7b10d1544ecc8503cdf363732622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c0c0f539341452a9ea60137b28cb249",
       "IPY_MODEL_92b941db825d40a5a88c8954cff53dbb",
       "IPY_MODEL_aa7a8cfe307c413b92607e4cf1754ffe"
      ],
      "layout": "IPY_MODEL_030fbd75958d40f3bfb6986b632ca4f3"
     }
    },
    "deb1e97276664d26a647a4c2681cac99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
